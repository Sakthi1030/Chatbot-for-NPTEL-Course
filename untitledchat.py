# -*- coding: utf-8 -*-
"""Untitledchat.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NNr4NWO7SdbJSuN6MI5N99E02jyp-qk5
"""

from google.colab import drive
drive.mount('/content/drive')

import os

root_path = '/content/drive/My Drive/'
print(os.listdir(root_path))

pdf_folder_path = '/content/drive/My Drive/NPT file'

for file_name in os.listdir(pdf_folder_path):
    print(file_name)

!pip install PyPDF2

from google.colab import drive
import PyPDF2
import os

def extract_text_from_pdf(pdf_path):
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        text = ""
        for page in range(len(reader.pages)):
            text += reader.pages[page].extract_text()
        return text

#text extraction
pdf_texts = {}
for file_name in os.listdir(pdf_folder_path):
    if file_name.endswith('.pdf'):
        pdf_path = os.path.join(pdf_folder_path, file_name)
        text = extract_text_from_pdf(pdf_path)
        pdf_texts[file_name] = text

for pdf_name, pdf_text in pdf_texts.items():
    print(f"Text from {pdf_name}: \n {pdf_text[:1000]}")

import json

with open('/content/drive/My Drive/pdf_texts.json', 'w') as f:
    json.dump(pdf_texts, f)

!pip install sentence-transformers faiss-cpu

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

# transformer model for embeddings
model = SentenceTransformer('all-MiniLM-L6-v2')

documents = list(pdf_texts.values())
embeddings = model.encode(documents, convert_to_numpy=True)

# Creating and populating the FAISS index
index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(embeddings)
faiss.write_index(index, '/content/drive/My Drive/index.faiss')

# Save the document
import pickle
with open('/content/drive/My Drive/documents.pkl', 'wb') as f:
    pickle.dump(list(pdf_texts.keys()), f)

!pip install transformers

from transformers import BertTokenizer, BertForQuestionAnswering, GPT2Tokenizer, GPT2LMHeadModel
import torch

# Load BERT for retrieval and GPT-2 for generation
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')
gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')

def retrieve_context_with_bert(query, context):
    # Tokenize the input and context
    inputs = bert_tokenizer.encode_plus(query, context, return_tensors='pt')
    with torch.no_grad():
        outputs = bert_model(**inputs)
        start_scores = outputs.start_logits
        end_scores = outputs.end_logits

    # Get the most likely start and end of the answer
    start_index = torch.argmax(start_scores)
    end_index = torch.argmax(end_scores) + 1

    # Convert tokens to the answer text
    answer = bert_tokenizer.convert_tokens_to_string(
        bert_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][start_index:end_index])
    )
    return answer

def chat_with_bot(query, context=None):
    if context:
        # Use BERT to find a relevant part of the context
        relevant_context = retrieve_context_with_bert(query, context)
    else:
        relevant_context = query

    # Tokenize the relevant context or query
    inputs = gpt2_tokenizer.encode(relevant_context, return_tensors='pt')

    # Check if tokenization was successful
    if inputs.numel() == 0:
        raise ValueError("Tokenization resulted in an empty tensor. Please check the input text.")

    # Generate a response
    outputs = gpt2_model.generate(inputs, max_length=150)

    response = gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response


# Example usage with a context
context = "Cybersecurity involves protecting computer systems, networks, and data from digital attacks."
query = "What is cybersecurity?"
print(chat_with_bot(query, context))

!pip install streamlit transformers
!wget -q -O ngrok.zip https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip
!unzip ngrok.zip

# Commented out IPython magic to ensure Python compatibility.
# %%writefile chatbot_app.py
# 
# import streamlit as st
# from transformers import pipeline
# 
# # Load the GPT-2 model (or any model you're using)
# generator = pipeline('text-generation', model='gpt2')
# 
# st.title("Chatbot with GPT-2")
# 
# # Create an input box for user input
# user_input = st.text_input("Ask something:")
# 
# if user_input:
#     # Generate the chatbot's response
#     response = generator(user_input, max_length=50)[0]['generated_text']
#     st.write(f"Chatbot: {response}")
#

!streamlit run chatbot_app.py & npx localtunnel --port 8501

!curl https://loca.lt/mytunnelpassword

!ls

!pip freeze > requirments.txt